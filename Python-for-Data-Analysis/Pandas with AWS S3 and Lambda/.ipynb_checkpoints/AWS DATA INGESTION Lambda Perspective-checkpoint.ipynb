{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# AWS DATA INGESTION - Lambda Perspective\n",
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "Make sure you install each of module / library \n",
    "\n",
    "* pip install 'module name' on your command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io \n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import awswrangler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a> <img src='img\\xl_ingest_pic_v2.png' width=\"1000\" /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event JSON Structure\n",
    "https://docs.aws.amazon.com/en_us/AmazonS3/latest/dev/notification-content-structure.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{  \n",
    "   \"Records\":[  \n",
    "      {  \n",
    "         \"eventVersion\":\"2.1\",\n",
    "         \"eventSource\":\"aws:s3\",\n",
    "         \"awsRegion\":\"us-west-2\",\n",
    "         \"eventTime\":The time, in ISO-8601 format, for example, 1970-01-01T00:00:00.000Z, when Amazon S3 finished processing the request,\n",
    "         \"eventName\":\"event-type\",\n",
    "         \"userIdentity\":{  \n",
    "            \"principalId\":\"Amazon-customer-ID-of-the-user-who-caused-the-event\"\n",
    "         },\n",
    "         \"requestParameters\":{  \n",
    "            \"sourceIPAddress\":\"ip-address-where-request-came-from\"\n",
    "         },\n",
    "         \"responseElements\":{  \n",
    "            \"x-amz-request-id\":\"Amazon S3 generated request ID\",\n",
    "            \"x-amz-id-2\":\"Amazon S3 host that processed the request\"\n",
    "         },\n",
    "         \"s3\":{  \n",
    "            \"s3SchemaVersion\":\"1.0\",\n",
    "            \"configurationId\":\"ID found in the bucket notification configuration\",\n",
    "            \"bucket\":{  \n",
    "               \"name\":\"bucket-name\",\n",
    "               \"ownerIdentity\":{  \n",
    "                  \"principalId\":\"Amazon-customer-ID-of-the-bucket-owner\"\n",
    "               },\n",
    "               \"arn\":\"bucket-ARN\"\n",
    "            },\n",
    "            \"object\":{  \n",
    "               \"key\":\"object-key\",\n",
    "               \"size\":object-size,\n",
    "               \"eTag\":\"object eTag\",\n",
    "               \"versionId\":\"object version if bucket is versioning-enabled, otherwise null\",\n",
    "               \"sequencer\": \"a string representation of a hexadecimal value used to determine event sequence, \n",
    "                   only used with PUTs and DELETEs\"\n",
    "            }\n",
    "         },\n",
    "         \"glacierEventData\": {\n",
    "            \"restoreEventData\": {\n",
    "               \"lifecycleRestorationExpiryTime\": \"The time, in ISO-8601 format, for example, 1970-01-01T00:00:00.000Z, of Restore Expiry\",\n",
    "               \"lifecycleRestoreStorageClass\": \"Source storage class for restore\"\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Handler Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from rawprod_to_csv import readxls_rawprod_convertcsv\n",
    "\n",
    "# import ptvsd\n",
    "\n",
    "# ptvsd.enable_attach(address=('0.0.0.0',5890),redirect_output=True)\n",
    "# ptvsd.wait_for_attach()\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    if event:\n",
    "        dateTimeObj = datetime.now() + timedelta(hours=1) #to capture timestamp event lambda read event upload\n",
    "        file_obj=event[\"Records\"][0] #to get file objet information from event s3 json\n",
    "        fileName=str(file_obj['s3']['object']['key']).replace(\"+\",\" \") #target filename object from json replace + string into space\n",
    "        bucket = str(file_obj['s3']['bucket']['name']).replace(\"+\",\" \") #target bucketname object from json\n",
    "    \n",
    "        try:\n",
    "            x=fileName\n",
    "            if x=='rsw/uploadhere-user1/rawtest1.xlsx' :\n",
    "                print(x)\n",
    "                readxls_rawprod_convertcsv(bucket, fileName, dateTimeObj)\n",
    "                print(\"go to code: readxls_rawprod_convertcsv is passed\")\n",
    "            else:\n",
    "                print(\"the uploaded format file is not sufficient\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps(str(e))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "# import s3fs\n",
    "\n",
    "\n",
    "def readxls_rawprod_convertcsv(bucket, fileName, dateTimeObj):\n",
    "    #define data to read\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket, Key=fileName)\n",
    "    file_obj = io.BytesIO(obj['Body'].read())\n",
    "\n",
    "    #initial statement to store some paramters\n",
    "    newdata = None\n",
    "    cols_skip = []\n",
    "    datarawcheck = None\n",
    "\n",
    "    ###Check excel file\n",
    "    try:\n",
    "        datarawcheck = pd.read_excel(file_obj, sheet_name='input', header=None)\n",
    "        print(\"bucket:\" + bucket + \" filename:\" + fileName)\n",
    "        print(\"Excel Openned\")\n",
    "    except Exception as e:\n",
    "        print(\"Can't open the file. Please check the S3 bucket\")\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    #checking is there any blank column we'll tag the position and skip them in next uploading\n",
    "    for i in range(len(datarawcheck.columns)):\n",
    "        if (((pd.isna(datarawcheck[i])).nunique()) == 1) & (((\n",
    "            (pd.isna(datarawcheck[i])).unique())[0]) == True):\n",
    "            cols_skip.append(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    #checking is there any blank rows for each rows we'll skip it\n",
    "    rows_skip = []\n",
    "    for i in range(len(datarawcheck.iloc[i])):\n",
    "        if (((pd.isna(datarawcheck.iloc[i])).nunique()) == 1) & (((\n",
    "            (pd.isna(datarawcheck.iloc[i])).unique())[0]) == True):\n",
    "            rows_skip.append(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # define unblank columns and rows\n",
    "    cols = [i for i in range(len(datarawcheck.columns)) if i not in cols_skip]\n",
    "    rows = rows_skip\n",
    "\n",
    "    del datarawcheck\n",
    "\n",
    "    newdata = pd.read_excel(file_obj,\n",
    "                            sheet_name='input',\n",
    "                            skiprows=rows,\n",
    "                            usecols=cols)\n",
    "    # newdata= pd.read_excel(file_obj, sheet_name='input')\n",
    "    newdata.reset_index()\n",
    "    newdata = newdata.dropna(subset=['site']) #to drop row data any blank on site info \n",
    "    newdata['date'] = pd.to_datetime(newdata['date']) #to make date as date format\n",
    "    print(newdata.head()) #to be print on log cloudwatch when we check it\n",
    "    if (newdata is not None or not newdata.empty):\n",
    "        # newchecking_date = min(newdata['date']) #just to check if user upload file with previous date data\n",
    "        # flag with timeupload and user upload\n",
    "        a = str(dateTimeObj)\n",
    "        b = fileName\n",
    "        print(a)\n",
    "        print(b)\n",
    "        newdata['timesupload'] = a\n",
    "        newdata['userupload'] = b\n",
    "\n",
    "        # create new filename\n",
    "        yr = str(dateTimeObj.year)\n",
    "        mo = str(dateTimeObj.month)\n",
    "        day = str(dateTimeObj.day)\n",
    "        hr = str(dateTimeObj.hour+7)\n",
    "        mn = str(dateTimeObj.minute)\n",
    "        sc = str(dateTimeObj.second)\n",
    "        up_filename = yr + mo + day + hr + mn + sc + '.csv'\n",
    "\n",
    "        target_bucket = 'mst-data-lab'\n",
    "        target_object = 'output/'+ up_filename\n",
    "        \n",
    "        # to define target bucket to load or put the data\n",
    "        csv_buffer = StringIO()\n",
    "        newdata.to_csv(csv_buffer, index=False) #store data as csv format\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        s3_resource.Object(target_bucket,target_object).put(Body=csv_buffer.getvalue())\n",
    "        \n",
    "        # after we put the csv format file we call API GLUE Workflow\n",
    "        client = boto3.client('glue')\n",
    "        response = client.start_workflow_run(Name='workflowname')  #workflowname\n",
    "        print('Lambda function is DONE')\n",
    "\n",
    "    else:\n",
    "        print(\"No new data inserted\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We can't just put the codes in the lambda, we must download by our self in Linux environment and collect them into one ZIP file together with the codes.py. After that we store it in a S3 bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
