{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# AWS DATA INGESTION\n",
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a> <img src='img\\architecture_v2.png' width=\"1000\" /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io \n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import s3fs\n",
    "# from fastparquet import write\n",
    "import awswrangler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Excel / Flat Files Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a> <img src='img\\xl_ingest_pic_v2.png' width=\"1000\" /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>site</th>\n",
       "      <th>prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>x</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>x</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date site  prod\n",
       "0 2019-01-01    x   200\n",
       "1 2019-01-02    x   100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define bucket address\n",
    "bucket_name = 'mst-lab-data'\n",
    "object_key = 'data/rawtest2.xlsx'\n",
    "\n",
    "# s3 = boto3.client('s3', aws_access_key_id=aws_id, aws_secret_access_key=aws_secret)\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "data = obj['Body'].read()\n",
    "df = pd.read_excel(io.BytesIO(data), encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target bucket to load the data\n",
    "target_bucket='abm-data-platform-s3-raw'\n",
    "target_object='rsw/test.csv'\n",
    "csv_buffer = io.StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(target_bucket,target_object ).put(Body=csv_buffer.getvalue());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "s3_url = 's3://blueprint-group-landzone/subgroup_1/datacatalog1/bucket.parquet.gzip'\n",
    "df.to_parquet(s3_url, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "write('output/outfile2.parq', df,compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bytes_to_write = df.to_csv(None,index=False ).encode()\n",
    "fs = s3fs.S3FileSystem()\n",
    "with fs.open('s3://blueprint-group-rawdata/subgroup_1-rawdata/robert_raw1_csv/raw.csv', 'wb') as f:\n",
    "    f.write(bytes_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.127\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print (boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 8, 19, 11, 18, 11, 350466)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "dateTimeObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019819111811.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yr= str(dateTimeObj.year)\n",
    "mo= str(dateTimeObj.month)\n",
    "day= str(dateTimeObj.day)\n",
    "hr= str(dateTimeObj.hour)\n",
    "mn= str(dateTimeObj.minute)\n",
    "sc= str(dateTimeObj.second)\n",
    "up_filename=yr+mo+day+hr+mn+sc+'.csv'\n",
    "up_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>site</th>\n",
       "      <th>prod</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>useropluad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>x</td>\n",
       "      <td>100</td>\n",
       "      <td>2019-08-12 15:13:29.411861</td>\n",
       "      <td>abm/uploadhere/raw1.xlsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date site  prod                  timestamp                useropluad\n",
       "0 2019-01-01    x   100 2019-08-12 15:13:29.411861  abm/uploadhere/raw1.xlsx"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df\n",
    "df2['timestamp']=dateTimeObj\n",
    "df2['useropluad']=object_key\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-08-13 12:35:18.057543'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=str(dateTimeObj)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-13 12:35:18.057543\n"
     ]
    }
   ],
   "source": [
    "print(dateTimeObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>site</th>\n",
       "      <th>prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>x</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date site  prod\n",
       "0 2019-01-01    x   100"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datarawcheck.columns = range(df.shape[1])\n",
    "datarawcheck\n",
    "cols_skip=[]\n",
    "# pd.isna(datarawcheck[2])\n",
    "for i in range(len(datarawcheck.columns)):\n",
    "\t\tif (((pd.isna(datarawcheck[i])).nunique()) == 1 ) & ((((pd.isna(datarawcheck[i])).unique())[0])==True):\n",
    "\t\t\tcols_skip.append(i)\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "cols_skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [i for i in range(len(df.columns)) if i not in cols_skip]\n",
    "# \trows=len(rows_skip)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_skip=[]\n",
    "for i in range(len(datarawcheck.iloc[i])):\n",
    "\tif (((pd.isna(datarawcheck.iloc[i])).nunique()) == 1 ) & ((((pd.isna(datarawcheck.iloc[i])).unique())[0])==True):\n",
    "\t\trows_skip.append(i)\n",
    "\telse:\n",
    "\t\tbreak\n",
    "rows_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows=len(rows_skip)\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "write('output/outfile2.parq', df,compression='GZIP')\n",
    "# write('outfile2.parq', df, row_group_offsets=[0, 10000, 20000], compression='GZIP', file_scheme='hive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "database = \"abm\"\n",
    "table_name = \"data\"\n",
    "s3_path = \"s3://lz-layer/abm/\"\n",
    "\n",
    "sql_query = \"SELECT * FROM mj LIMIT 20\" \n",
    "db_name = \"test-db\"\n",
    "s3_output_bucket = \"s3://lz-layer/abm/\"\n",
    "\n",
    "# a = pg.read_glue(sql_query,db_name,s3_output_bucket,region='ap-southeast-1',key=aws_id,secret=aws_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>site</th>\n",
       "      <th>land_clearing</th>\n",
       "      <th>top_soil_removal</th>\n",
       "      <th>dump_to_rehabilitation</th>\n",
       "      <th>over_burden</th>\n",
       "      <th>overburden_removal_plan</th>\n",
       "      <th>rain</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>slippery</th>\n",
       "      <th>...</th>\n",
       "      <th>blc_stockpile</th>\n",
       "      <th>coal_barging_olc_to_bg</th>\n",
       "      <th>coal_barging_blc_to_bg</th>\n",
       "      <th>coal_barging</th>\n",
       "      <th>coal_barging_plan</th>\n",
       "      <th>coal_shipping_actual</th>\n",
       "      <th>coal_shipping_plan</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>MFA-MBH</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>13059.556</td>\n",
       "      <td>16.0</td>\n",
       "      <td>42.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5618.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5053.368581</td>\n",
       "      <td>2217</td>\n",
       "      <td>5053.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>MFA-MBH</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8214.0</td>\n",
       "      <td>13059.556</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4</td>\n",
       "      <td>12.8</td>\n",
       "      <td>...</td>\n",
       "      <td>16118.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5053.368581</td>\n",
       "      <td>3925</td>\n",
       "      <td>5053.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>MFA-MBH</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6813.0</td>\n",
       "      <td>13059.556</td>\n",
       "      <td>6.3</td>\n",
       "      <td>44.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21118.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5053.368581</td>\n",
       "      <td>3347</td>\n",
       "      <td>5053.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>MFA-MBH</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>13059.556</td>\n",
       "      <td>14.8</td>\n",
       "      <td>39</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23877.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5053.368581</td>\n",
       "      <td>823</td>\n",
       "      <td>5053.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>MFA-MBH</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1896.0</td>\n",
       "      <td>13059.556</td>\n",
       "      <td>20.4</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>...</td>\n",
       "      <td>28911.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5053.368581</td>\n",
       "      <td>2196</td>\n",
       "      <td>5053.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     site  land_clearing  top_soil_removal  \\\n",
       "0 2017-01-01  MFA-MBH           0.49               0.0   \n",
       "1 2017-01-02  MFA-MBH           1.08               0.0   \n",
       "2 2017-01-03  MFA-MBH           0.33               0.0   \n",
       "3 2017-01-04  MFA-MBH           1.02               0.0   \n",
       "4 2017-01-05  MFA-MBH           0.44               0.0   \n",
       "\n",
       "   dump_to_rehabilitation  over_burden  overburden_removal_plan  rain  \\\n",
       "0                       0       3384.0                13059.556  16.0   \n",
       "1                       0       8214.0                13059.556   2.2   \n",
       "2                       0       6813.0                13059.556   6.3   \n",
       "3                       0       1752.0                13059.556  14.8   \n",
       "4                       0       1896.0                13059.556  20.4   \n",
       "\n",
       "  rainfall  slippery ...   blc_stockpile  coal_barging_olc_to_bg  \\\n",
       "0     42.8       1.0 ...         5618.00                     0.0   \n",
       "1        4      12.8 ...        16118.00                     0.0   \n",
       "2     44.2       0.0 ...        21118.00                     0.0   \n",
       "3       39       9.0 ...        23877.56                     0.0   \n",
       "4     53.2       3.4 ...        28911.96                     0.0   \n",
       "\n",
       "   coal_barging_blc_to_bg  coal_barging  coal_barging_plan  \\\n",
       "0                     0.0           0.0        5053.368581   \n",
       "1                     0.0           0.0        5053.368581   \n",
       "2                     0.0           0.0        5053.368581   \n",
       "3                     0.0           0.0        5053.368581   \n",
       "4                     0.0           0.0        5053.368581   \n",
       "\n",
       "   coal_shipping_actual  coal_shipping_plan  Year  Month  Day  \n",
       "0                  2217              5053.0  2017      1    1  \n",
       "1                  3925              5053.0  2017      1    2  \n",
       "2                  3347              5053.0  2017      1    3  \n",
       "3                   823              5053.0  2017      1    4  \n",
       "4                  2196              5053.0  2017      1    5  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Year']= pd.to_datetime(df['date']).dt.strftime(\"%Y\")\n",
    "df['Month']= pd.to_datetime(df['date']).dt.strftime(\"%m\")\n",
    "df['Month']=pd.to_numeric(df['Month'])\n",
    "df['Day']= pd.to_datetime(df['date']).dt.strftime(\"%d\")\n",
    "\n",
    "df['Day']=pd.to_numeric(df['Day'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sharepoint List Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a> <img src='pics\\sharepoint challenge.png' width=\"600\" /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Cloud Watch by schedule will trigger the Lambda function.\n",
    "\n",
    ">Lambda function will call API – Sharepoint to read targeted list table to be ingested into AWS Data Platform and to process some actions as below : \n",
    "\n",
    ">The successfully uploaded file will trigger Lambda functions to process some actions as below : \n",
    ">   1. Re-check the template structure.\n",
    " \n",
    ">   2. Filter the data who is not in landing zone –S3 bucket-folder yet.\n",
    " \n",
    ">   3. Convert the processed data into parquet format.\n",
    "\n",
    ">All the processed raw data will be loaded / inserted into targeted landing_zone S3 bucket-folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQL DB Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a> <img src='pics\\sqldb challenge.png' width=\"600\" /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Add connection for each SQL db using JDBC into AWS Data Platform as data catalog in a landing zonee database  (in targeted landing zone  S3 bucket-folder).\n",
    "\n",
    ">By Schedule, AWS Glue will read the table and generate the hash-key unique.\n",
    "\n",
    ">The python codes will recheck and filter the hash-key unique which is not in the existing loaded table already.\n",
    "\n",
    ">Convert the processed data into parquet format.\n",
    "\n",
    ">All the processed raw data will be loaded / inserted into targeted landing_zone S3 bucket-folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
